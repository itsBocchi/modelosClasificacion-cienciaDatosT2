{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab6b403",
   "metadata": {},
   "source": [
    "<head>\n",
    "<b><center>CIENCIA DE DATOS</center>\n",
    "\n",
    "<center>TALLER 2</center>\n",
    "<center>2024-1</center>\n",
    "\n",
    "<center>Profesor: Gabriel Jara </center><br>\n",
    "<center>Integrantes: Fernando Salgado, Cristián González</center><br>\n",
    "</head>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6148c",
   "metadata": {},
   "source": [
    "#### Introducción\n",
    "---\n",
    "El presente informe tiene como objetivo presentar el desarrollo del taller 2 de la asignatura de Ciencia de Datos, el cual consiste en el desarrollo de un modelo de clasificación para predecir si un estudiante de la Universidad Técnica Federico Santa María (UTFSM) sede Viña del Mar, se titulará o no. Para ello, se cuenta con un dataset anonimizados ingresado en entre los años 2004 y 2009 y su estado de titulación al 2014.\n",
    "\n",
    "##### Archivos\n",
    "---\n",
    "- `taller_2.ipynb`: Jupyter Notebook con el desarrollo del taller.\n",
    "- `Taller_2_Titulacion_DatosTaller.csv`: Dataset con los datos de los estudiantes.\n",
    "- `Taller_2_Titulacion_Evaluacion.csv`: Dataset con los datos de evaluación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ed1a1",
   "metadata": {},
   "source": [
    "#### Preprocesamiento de datos\n",
    "---\n",
    "A continuación se realizará el preprocesamiento de los datos, el cual consiste en la limpieza y transformación de los datos para su posterior análisis. Para ello, se realizará la carga de los datos, la eliminación de columnas innecesarias, la eliminación de filas con datos faltantes, la transformación de variables categóricas a numéricas y la normalización de los datos.\n",
    "Se debió cambiar el nombre de los archivos de origen quitando tilde en las vocales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517b6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del conjunto de datos de entrenamiento:\n",
      "Index(['Id', 'MAT_1SEM_PROM', 'FIS_1SEM_PROM', 'ING_1SEM_PROM', 'ACTF_1SEM_A',\n",
      "       'ACTF_1SEM_R', 'OTRANS_1SEM_A', 'OTRANS_1SEM_R', 'OTRANS_1SEM_PROM',\n",
      "       'ESP_1SEM_A', 'ESP_1SEM_R', 'ESP_1SEM_PROM', 'INS_1SEM', 'PROM_1SEM',\n",
      "       'MAT_2SEM_PROM', 'FIS_2SEM_PROM', 'ING_2SEM_PROM', 'ACTF_2SEM_A',\n",
      "       'ACTF_2SEM_R', 'OTRANS_2SEM_A', 'OTRANS_2SEM_R', 'OTRANS_2SEM_PROM',\n",
      "       'ESP_2SEM_A', 'ESP_2SEM_R', 'ESP_2SEM_PROM', 'INS_2SEM', 'PROM_2SEM',\n",
      "       'INSC_AnO', 'PROM_AnO', 'GENERO', 'NOM_CARRERA', 'TIPO_CARRERA',\n",
      "       'nombre_nacionalidad', 'descripcion_situacion_egreso_postulante',\n",
      "       'nombre_comuna_EM', 'nombre_provincia_EM', 'nombre_region_EM',\n",
      "       'nombre_dependencia_EM', 'nombre_grupo_dependencia_EM',\n",
      "       'descripcion_rama_educacional_EM', 'nombre_secretaria_admision',\n",
      "       'numero_estado_civil', 'descripcion_trabajo_remunerado',\n",
      "       'numero_horario_trabajo', 'descripcion_proseguir_estudios',\n",
      "       'descripcion_jefe_familia',\n",
      "       'descripcion_fuente_financiamiento_estudio_superior',\n",
      "       'descripcion_fuente_financiamiento_estudio_superior2',\n",
      "       'inferior_ingreso_bruto_fam', 'superior_ingreso_bruto_fam',\n",
      "       'nombre_cobertura_salud', 'descripcion_viven_padres',\n",
      "       'descripcion_nivel_educacion_padre',\n",
      "       'descripcion_nivel_educacion_madre',\n",
      "       'descripcion_situacion_ocupacional_padre',\n",
      "       'descripcion_situacion_ocupacional_madre',\n",
      "       'descripcion_tipo_organismo_trabajan_padre',\n",
      "       'descripcion_tipo_organismo_trabajan_madre',\n",
      "       'descripcion_ocupacion_principal_padre',\n",
      "       'descripcion_ocupacion_principal_madre',\n",
      "       'descripcion_rama_actividad_padre', 'descripcion_rama_actividad_madre',\n",
      "       'horas_promedio_trabajo', 'cantidad_personas_grupo_familiar',\n",
      "       'cuantos_trabajan_grupo_familiar', 'cuantos_estudian_grupo_pre_basica',\n",
      "       'cuantos_estudian_grupo_basica', 'cuantos_estudian_grupo_media_1_3',\n",
      "       'cuantos_estudian_grupo_media_4', 'cuantos_estudian_grupo_superior',\n",
      "       'cuantos_estudian_grupo_otras', 'ptje_nem', 'ptje_leng', 'ptje_mate',\n",
      "       'ptje_hycs', 'ptje_cien', 'Rotulo_Titulo'],\n",
      "      dtype='object')\n",
      "\n",
      "Valores nulos en el conjunto de datos de entrenamiento:\n",
      "Id                  0\n",
      "MAT_1SEM_PROM     836\n",
      "FIS_1SEM_PROM    1207\n",
      "ING_1SEM_PROM     845\n",
      "ACTF_1SEM_A      1331\n",
      "                 ... \n",
      "ptje_leng           0\n",
      "ptje_mate           0\n",
      "ptje_hycs           0\n",
      "ptje_cien           0\n",
      "Rotulo_Titulo       0\n",
      "Length: 77, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos principal\n",
    "try:\n",
    "    df_train = pd.read_csv('Taller_2_Titulacion_DatosTaller.csv', encoding='latin1', sep=';')\n",
    "except UnicodeDecodeError:\n",
    "    df_train = pd.read_csv('Taller_2_Titulacion_DatosTaller.csv', encoding='iso-8859-1', sep=';')\n",
    "\n",
    "# Cargar el conjunto de datos de evaluación\n",
    "\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Columnas del conjunto de datos de entrenamiento:\")\n",
    "print(df_train.columns)\n",
    "\n",
    "# Verificar si 'Rotulo_Titulo' está en las columnas\n",
    "estado_titulacion_col = 'Rotulo_Titulo'\n",
    "if (estado_titulacion_col not in df_train.columns) or ('Id' not in df_train.columns):\n",
    "    raise KeyError(f\"La columna '{estado_titulacion_col}' o 'Id' no se encuentra en el DataFrame.\")\n",
    "\n",
    "# Convertir la variable de estado de titulación a binaria\n",
    "df_train[estado_titulacion_col] = df_train[estado_titulacion_col].map({'SI': 1, 'NO': 0})\n",
    "\n",
    "# Separar características y variable objetivo\n",
    "X = df_train.drop(columns=[estado_titulacion_col, 'Id'])\n",
    "y = df_train[estado_titulacion_col]\n",
    "\n",
    "print(\"\\nValores nulos en el conjunto de datos de entrenamiento:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "\n",
    "# Eliminar columnas con todos los valores faltantes en el conjunto de entrenamiento\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Codificación de variables categóricas antes de la imputación\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Imputar valores nulos en el conjunto de entrenamiento\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Escalado de características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e95ba",
   "metadata": {},
   "source": [
    "#### Validación de datos\n",
    "Se eligio el algoritmo GridSearchCV que aplica la estrategia de validacion K-Fold con k=5 (cv), debido a que se tiene un dataset con un tamaño considerable, su velocidad de ejecución y nos facilita fijar distintos parametros. Consideramos usar validacion simple y Leave One Out Cross-Validation (LOOCV), pero se descarto porque el primero no es recomendable para datasets grandes y el segundo es muy costoso computacionalmente.\n",
    "El codigo se encuentra en la sección de \"Modelo de clasificación\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c93e3",
   "metadata": {},
   "source": [
    "#### Modelos de clasificación\n",
    "Para realizar la clasificación se evaluaron los modelos de regresión logística y Random Forest, ambos modelos son ampliamente utilizados para problemas de clasificación.\n",
    "Se utilizó la metrica de accuracy para evaluar los modelos, que mide la proporción de predicciones correctas que el modelo realiza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a04be",
   "metadata": {},
   "source": [
    "##### Iteracion 1\n",
    "---\n",
    "Regresión Logística\n",
    "<ul>Parametros:\n",
    "<li>max_iter=1000</li>\n",
    "<li>random_state=181</li>\n",
    "</ul>\n",
    "<ul>Variables:\n",
    "<li>'C': [0.1, 1, 10]</li>\n",
    "<li>'penalty': ['l2']</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5660e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para Regresión Logística: {'C': 0.1, 'penalty': 'l2'}\n",
      "Precisión media usando los mejores parámetros: 0.7034425005216742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el modelo\n",
    "model = LogisticRegression(max_iter=1000, random_state=181)\n",
    "\n",
    "# Definir los parámetros para búsqueda exhaustiva\n",
    "params = {'C': [0.1, 1, 10], 'penalty': ['l2']}\n",
    "\n",
    "# Búsqueda exhaustiva para encontrar los mejores parámetros\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='accuracy')\n",
    "grid.fit(X_scaled, y)\n",
    "\n",
    "# Mostrar los mejores parámetros y la precisión media\n",
    "print(\"Mejores parámetros para Regresión Logística:\", grid.best_params_)\n",
    "print(\"Precisión media usando los mejores parámetros:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac99ba9",
   "metadata": {},
   "source": [
    "##### Iteracion 2\n",
    "---\n",
    "Regresión Logística\n",
    "<ul>Parametros:\n",
    "<li>max_iter=1000</li>\n",
    "<li>random_state=42</li>\n",
    "</ul>\n",
    "<ul>Variables:\n",
    "<li>'C': [0.1, 1, 10]</li>\n",
    "<li>'penalty': ['l1']</li>\n",
    "<li>'solver': ['liblinear']</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3983d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para Regresión Logística: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Precisión media usando los mejores parámetros: 0.7329247800731442\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Definir los parámetros para búsqueda exhaustiva\n",
    "params = {'C': [0.1, 1, 10], 'penalty': ['l1'], 'solver': ['liblinear']}\n",
    "\n",
    "# Búsqueda exhaustiva para encontrar los mejores parámetros\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='accuracy')\n",
    "grid.fit(X_scaled, y)\n",
    "\n",
    "# Mostrar los mejores parámetros y la precisión media\n",
    "print(\"Mejores parámetros para Regresión Logística:\", grid.best_params_)\n",
    "print(\"Precisión media usando los mejores parámetros:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee843c",
   "metadata": {},
   "source": [
    "##### Iteracion 3\n",
    "---\n",
    "Random Forest\n",
    "<ul>Parametros:\n",
    "<li>random_state=823</li>\n",
    "</ul>\n",
    "<ul>Variables:\n",
    "<li>'max_depth': [None, 10, 20, 30]</li>\n",
    "<li>'max_features': ['sqrt', 'log2']</li>\n",
    "<li>'criterion':['gini', 'entropy', 'log_loss']</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189a4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para Random Forest: {'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt'}\n",
      "Precisión media usando los mejores parámetros: 0.7518044435658353\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "model_rf = RandomForestClassifier(random_state=823)\n",
    "\n",
    "# Definir los parámetros para búsqueda exhaustiva\n",
    "params_rf = {'max_depth': [None, 10, 20, 30],'max_features': ['sqrt', 'log2'],'criterion':['gini', 'entropy', 'log_loss']}\n",
    "\n",
    "# Búsqueda exhaustiva para encontrar los mejores parámetros\n",
    "grid_rf = GridSearchCV(estimator=model_rf, param_grid=params_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X_scaled, y)\n",
    "\n",
    "# Mostrar los mejores parámetros y la precisión media\n",
    "print(\"Mejores parámetros para Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Precisión media usando los mejores parámetros:\", grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad4fea",
   "metadata": {},
   "source": [
    "##### Iteracion 4\n",
    "---\n",
    "Random Forest\n",
    "<ul>Parametros:\n",
    "<li>random_state=42</li>\n",
    "</ul>\n",
    "<ul>Variables:\n",
    "<li>'n_estimators': [50, 100, 200]</li>\n",
    "<li>'max_depth': [None, 10, 20, 30]</li>\n",
    "<li>'max_features': ['sqrt', 'log2']</li>\n",
    "<li>'criterion':['gini', 'entropy', 'log_loss']</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb1b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para Random Forest: {'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "Precisión media usando los mejores parámetros: 0.7557779534996101\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definir los parámetros para búsqueda exhaustiva\n",
    "params_rf = {'n_estimators': [50, 100, 200],'max_depth': [None, 10, 20, 30],'max_features': ['sqrt', 'log2'],'criterion':['gini', 'entropy', 'log_loss']}\n",
    "\n",
    "# Búsqueda exhaustiva para encontrar los mejores parámetros\n",
    "grid_rf = GridSearchCV(estimator=model_rf, param_grid=params_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X_scaled, y)\n",
    "\n",
    "# Mostrar los mejores parámetros y la precisión media\n",
    "print(\"Mejores parámetros para Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Precisión media usando los mejores parámetros:\", grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942de3cd",
   "metadata": {},
   "source": [
    "#### Evaluación sobre casos nuevos\n",
    "---\n",
    "Se realiza la evaluación del modelo seleccionado sobre un conjunto de datos proporcionados en el archivo Taller_2_Titulacion_Evaluacion.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d983ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n",
      "C:\\Users\\borrar\\AppData\\Local\\Temp\\ipykernel_12224\\72123807.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_eval[col] = 0\n"
     ]
    }
   ],
   "source": [
    "# Cargar el conjunto de datos de evaluación\n",
    "try:\n",
    "    df_eval = pd.read_csv('Taller_2_Titulacion_Evaluacion.csv', encoding='latin1', sep=';')\n",
    "except UnicodeDecodeError:\n",
    "    df_eval = pd.read_csv('Taller_2_Titulacion_Evaluacion.csv', encoding='iso-8859-1', sep=';')\n",
    "\n",
    "# Preprocesamiento del conjunto de datos de evaluación\n",
    "X_eval = df_eval.drop(columns=['Id'])\n",
    "X_eval = pd.get_dummies(X_eval)\n",
    "\n",
    "# Asegurarse de que X_eval tenga las mismas columnas que X\n",
    "missing_cols = set(X.columns) - set(X_eval.columns)\n",
    "for col in missing_cols:\n",
    "    X_eval[col] = 0\n",
    "X_eval = X_eval[X.columns]\n",
    "\n",
    "# Imputar valores nulos\n",
    "X_eval = pd.DataFrame(imputer.transform(X_eval), columns=X_eval.columns)\n",
    "\n",
    "# Escalar características\n",
    "X_eval_scaled = pd.DataFrame(scaler.transform(X_eval), columns=X_eval.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eaa68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en clasificación_título.txt\n"
     ]
    }
   ],
   "source": [
    "# Utilizar el modelo con los mejores parámetros obtenidos\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Realizar la predicción sobre el conjunto de evaluación\n",
    "y_pred = best_model.predict(X_eval_scaled)\n",
    "\n",
    "# Generar archivo de salida\n",
    "with open('clasificación_título.txt', 'w') as f:\n",
    "    for id, prediction in zip(df_eval['Id'], y_pred):\n",
    "        f.write(f\"{id},{['NO', 'SI'][prediction]}\\n\")\n",
    "\n",
    "print(\"Predicciones guardadas en clasificación_título.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
